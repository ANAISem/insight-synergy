# API und Server-Einstellungen
HOST=0.0.0.0
PORT=8000
DEBUG=true
API_PREFIX=/api
SECRET_KEY=supersecretkey-changethisimmediately

# CORS-Einstellungen
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:8080

# LLM-Einstellungen
# Wähle einen der folgenden Provider: openai, huggingface, local
LLM_PROVIDER=openai

# OpenAI-Einstellungen (für LLM_PROVIDER=openai)
OPENAI_API_KEY=sk-your-openai-api-key
LLM_MODEL_NAME=gpt-3.5-turbo-1106
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1024

# HuggingFace-Einstellungen (für LLM_PROVIDER=huggingface)
# HUGGINGFACE_API_KEY=hf_your-huggingface-api-key
# LLM_MODEL_NAME=mistralai/Mixtral-8x7B-Instruct-v0.1

# HuggingFace Inference API Endpoint (optional)
# HUGGINGFACE_INFERENCE_ENDPOINT=https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1

# Lokale LLM-Einstellungen (für LLM_PROVIDER=local)
# USE_OLLAMA=true
# OLLAMA_BASE_URL=http://localhost:11434
# LLM_MODEL_NAME=mistral

# ODER: llama.cpp für lokale LLMs
# USE_LLAMACPP=true
# LLAMACPP_MODEL_PATH=/path/to/your/model.gguf

# Vector-DB-Einstellungen
VECTOR_DB_PROVIDER=chroma
VECTOR_DB_PERSIST_DIRECTORY=./nexus_data

# Embedding-Modell-Einstellungen
EMBEDDING_MODEL_PROVIDER=sentence-transformers
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
# Für mehrsprachige Anwendungen:
# EMBEDDING_MODEL_NAME=paraphrase-multilingual-MiniLM-L12-v2

# Logging-Einstellungen
LOG_LEVEL=INFO
LOG_FILE=nexus_backend.log

# Performance-Einstellungen
CACHE_TTL_SECONDS=3600
BATCH_SIZE=32
MAX_WORKERS=4

# WebSocket-Einstellungen
WEBSOCKET_MAX_SIZE=16777216  # 16MB maximale Nachrichtengröße
WEBSOCKET_PING_INTERVAL=20  # Ping alle 20 Sekunden für Keep-Alive
WEBSOCKET_PING_TIMEOUT=60  # Timeout nach 60 Sekunden ohne Antwort 